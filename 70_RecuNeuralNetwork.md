# List

- [List of Awesome Recurrent Neural Networks](https://github.com/kjw0612/awesome-rnn): Jiwon Kim

- [List of NLP를 위한 딥러닝 가이드](http://docs.likejazz.com/deep-learning-for-nlp/#nlp) : Sang-Kil Park 블로그글

# Article / Post

- [ppt:홍콩과기대 김성훈 교수](https://docs.google.com/presentation/d/1HtcH9Kam8Lmv-QIT_fJ5rXleatgog7pgMkXVwzrYkY4/edit#slide=id.g216584368d_0_205)

- <del>[초보자를 위한 RNNs과 LSTM 가이드](https://deeplearning4j.org/kr/lstm): Deeplearning4J 번역 자료 </del>
  - [DL4J와 RNNs (Recurrent Neural Networks)](https://deeplearning4j.org/kr/usingrnns)

 - [Understanding LSTM and its diagrams](https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714)

* [Building Jarvis AI with Natural Language Processing](https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more) : By Mark Zuckerburg, CEO at Facebook.

* [Deep Learning Research Review: Natural Language Processing](http://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html)

* Understanding LSTM Networks: colah's Blog [원문]((http://colah.github.io/posts/2015-08-Understanding-LSTMs/), <del>[번역1](https://brunch.co.kr/@chris-song/9)</del>, [번역2](http://whydsp.org/280)

- [ppt: Introduction For seq2seq(sequence to sequence) and RNN](https://www.slideshare.net/HyeminAhn/introduction-for-seq2seqsequence-to-sequence-and-rnn)

- [4 APPROACHES TO NATURAL LANGUAGE PROCESSING & UNDERSTANDING](http://www.topbots.com/4-different-approaches-natural-language-processing-understanding): LP의 4가지 다른 접근방법
1. Distributional : 최근 유행하는 ML이 여기죠~ 폭은 넓힐 수 있지만, 깊이는 잡지 못함
2. Frame-based: 마빈 민스키 ... 논리적 semantics 에 강점. 확고한 supervision이 존재해야 한다는 큰 단점이~
3. Model-theoretical: Q/A와 rich semantics의 장점. labor-intensive and narrow in scope (프레임 기반보다 더함)
4. Interactive learning: language as a cooperative game between speaker and listener ... 이게 앞으로 제일 promising 하지 않을까?
```
이 개념이 재미있네요.
1. Syntax – what is grammatical? : “no compiler errors”
2. Semantics – what is the meaning?: “no implementation bugs”
3. Pragmatics – what is the purpose or goal?: “implemented the right algorithm.” ... 사실 목표는 여기!
```
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/): RNN의 선능

- [RNN Regularizations](http://nmhkahn.github.io/RNN-Regularizations): RNN 오버피팅 문제 해결법 정리

- RECURRENT NEURAL NETWORKS TUTORIAL
  - [PART 1](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/): INTRODUCTION TO RNNS [[번역]](http://aikorea.org/blog/rnn-tutorial-1/)
  - [PART 2](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/): IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO [[번역]](http://aikorea.org/blog/rnn-tutorial-2/)
  - [PART 3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/): BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS [[번역]](http://aikorea.org/blog/rnn-tutorial-3/)
  - [PART 4](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/): IMPLEMENTING A GRU/LSTM RNN WITH PYTHON AND THEANO [[번역]]()


- Unfolding RNNs By `Suriyadeepan Ram`

  - [PART1](http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/): Concepts and Architectures
  - [PART2](http://suriyadeepan.github.io/2017-02-13-unfolding-rnn-2/): Vanilla, GRU, LSTM RNNs from scratch in Tensorflow

# Tutorial

- [음성인식 관련 텐서플로우 RNN Tutorial](https://svds.com/tensorflow-rnn-tutorial/): TensorFlow RNN Tutorial, 2017.03.23

* [모두의 연구소:RNN과 LSTM - 쫄지말자 딥러닝](http://www.modulabs.co.kr/DeepLAB_library/11886)

# Implemetation

* [Anyone Can Learn To Code an LSTM-RNN in Python](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)

- [모두를 위한 딥러닝 RNN자료](https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-12-1-hello-rnn.py): lab-12-1-hello-rnn.py

- [Hello_LSTM](https://github.com/skyer9/hello_lstm/blob/master/hello_lstm.py)

- [hello_sequence.py](https://gist.github.com/pannous/b3f8ab944a85b33e694de21c6ded029e): simple Sequence-to-sequence model with an attention mechanism

# Papers
- [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619)
